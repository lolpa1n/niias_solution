{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Необходимые библиотеки\n",
    "\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30dd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Путь для тестовых изображений\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"dataset-here\") #путь до датасета\n",
    "\n",
    "test_image_path = ROOT / \"test/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6337d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Класс датасета\n",
    "\n",
    "class NiiasDatasetSampleSolution(Dataset):\n",
    "    def __init__(self, df, folder_path, transform=None):\n",
    "        self.df = df\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.df.iloc[index]['img_name']\n",
    "        image = np.array(Image.open(os.path.join(self.folder_path, name)).convert(\"RGB\"))\n",
    "        img_h, img_w, _ = image.shape\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image)\n",
    "            image = augmentations[\"image\"]\n",
    "        \n",
    "        image = image.transpose(2, 0, 1)\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.float()/255\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'name': name,\n",
    "            'img_h': img_h,\n",
    "            'img_w': img_w,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfa64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_PARAMETERS = {\n",
    "    'IMAGE_HEIGHT': 512,\n",
    "    'IMAGE_WIDTH': 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15843d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=GLOBAL_PARAMETERS['IMAGE_HEIGHT'],width=GLOBAL_PARAMETERS['IMAGE_WIDTH']),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16148aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = sorted(test_image_path.glob(\"*.png\"))\n",
    "solution_names = [i.name for i in test_path]\n",
    "\n",
    "solution_df = pd.DataFrame(solution_names, columns=['img_name'])\n",
    "solution_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Лоадер\n",
    "\n",
    "solution_ds = NiiasDatasetSampleSolution(\n",
    "        df=solution_df,\n",
    "        folder_path=test_image_path,\n",
    "        transform=solution_transforms)\n",
    "\n",
    "solution_loader = DataLoader(\n",
    "    solution_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acffe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Используемые модели\n",
    "\n",
    "models = [\n",
    "    smp.Unet('resnet34', encoder_weights='imagenet', classes=4, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16]),\n",
    "    smp.Unet('resnext50_32x4d', encoder_weights='imagenet', classes=4, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16]),\n",
    "    smp.Unet('efficientnet-b2', encoder_weights='imagenet', classes=4, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пути до чекпоинтов моделей\n",
    "\n",
    "checkpoints = [\n",
    "    '../models/unet-resnet34.pt',\n",
    "    '../models/unet-resnext50.pt',\n",
    "    '../models/unet-efficientnet-b2.pt',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузка моделей в память\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "loaded_models = []\n",
    "\n",
    "for model, check in zip(models, checkpoints):\n",
    "    model = torch.load(check, map_location=device)\n",
    "    model.eval\n",
    "    loaded_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477280b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предикт ансамблем моделей\n",
    "\n",
    "!rm -rf sample_solution\n",
    "!mkdir sample_solution\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n, batch in enumerate(tqdm(solution_loader)):\n",
    "\n",
    "        template = torch.zeros(1, 4, 512, 1024)\n",
    "        \n",
    "        for model, thres in zip(loaded_models, [0.33, 0.33, 0.33]):\n",
    "            \n",
    "            predictions = model.predict((batch['image'].to(device)))\n",
    "            template += predictions.cpu().detach().numpy() * thres\n",
    "            \n",
    "        template = torch.argmax(template, dim=1)\n",
    "        template = template.cpu().squeeze(0).numpy()\n",
    "\n",
    "        template[template == 1] = 6\n",
    "        template[template == 2] = 7\n",
    "        template[template == 3] = 10\n",
    "\n",
    "        prediction_mask_gray = Image.fromarray(template.astype(np.uint8))\n",
    "        prediction_mask_gray = prediction_mask_gray.resize((batch['img_w'], batch['img_h']), Image.NEAREST)\n",
    "        prediction_mask_gray.save(os.path.join(\"sample_solution\", f\"{batch['name'][0]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3604ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test_path = Path('./sample_solution/')\n",
    "mask_test_path = sorted(mask_test_path.glob(\"*.png\"))\n",
    "len(mask_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посмотрим случайный предикт\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_path = str(random.sample(mask_test_path, 1)[0])\n",
    "random_mask = cv2.imread(random_path)\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.imshow(random_mask*30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
